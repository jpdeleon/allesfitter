#!/usr/bin/env python
"""
Usage
$python prepare_allesfit.py -toi 1097 -s all -e 120
$python prepare_allesfit.py -tic 273586149 -s -1 -p qlp
$python prepare_allesfit.py -name 'HIP 67522' -o -i --debug

Uses parameter from TOI/CTIO/NExSci databse and
creates a directory with the files needed to run allesfitter:
1. params.csv
2. settings.csv
3. run.py
4. params_star.csv
5. mission.csv e.g. tess.csv, k2.csv, kepler.csv
======
* for precise transit transit timing, some parameters can be fixed
* limb darkening can be fixed to theoretical values derived using ~ldtk~ limbdark;
  assumes feh=(0,0.1) dex if feh is not available
* uses tess-point to check if target was observed by TESS
(useful to know even if `lightkurve.search_lightcurve` returned None)
* uses aliases (K2 name --> EPIC)
https://exoplanetarchive.ipac.caltech.edu/docs/sysaliases.html
======
"""
import sys
from typing import Tuple
from argparse import ArgumentParser
from pathlib import Path
from math import ceil
import numpy as np
import lightkurve as lk
import astropy.units as u
import pandas as pd
import matplotlib.pyplot as plt
from scipy.stats import anderson
from astropy.coordinates import SkyCoord
from allesfitter import allesclass  # , config, nested_sampling_output, general_output
from loguru import logger
# from ldtk import LDPSetCreator, BoxcarFilter
from tess_stars2px import tess_stars2px_function_entry
from allesfitter.utils.scripting import (
    catalog_info_TIC,
    get_tfop_info,
    get_tois,
    get_ctois,
    rho_from_mr,
    as_from_rhop,
    a_from_rhoprs,
    get_nexsci,
    get_name_aliases,
    get_tdur,
    get_rsuma,
)
logger.remove() 
log_format = "<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{message}</level>"
logger.add(sys.stderr, format=log_format)
try:
    import limbdark as ld
except Exception as e:
    logger.error(f"Error: {e}")
    command = (
        "pip install git+https://github.com/john-livingston/limbdark.git#egg=limbdark"
    )
    raise ModuleNotFoundError(command)

assert lk.__version__[0] == "2"

filter_widths = {
    "gp": (400, 550),
    "V": (480, 600),
    "rp": (560, 700),
    "ip": (700, 820),
    "zs": (825, 920),
    "I+z": (720, 1030),
    "tess": (585, 1050),
}

#home = Path.home()
#sys.path.insert(0, f"{home}/github/research/project/young_ttvs/code")

cols = ["time", "flux", "flux_err"]

Nsamples = 10_000
planets = "b c d e f g h i j k".split()
quartiles_1sig = [16.0, 50.0, 84.0]  # 1-sigma
quartiles_2sig = [2.275, 50.0, 97.725]  # 2-sigma
quartiles_3sig = [0.135, 50.0, 99.865]  # 3-sigma

def catalog_info_name(df) -> Tuple:
    Teff, Teff_err = df["st_teff"].astype(float), np.sqrt(
        df["st_tefferr1"] ** 2 + df["st_tefferr2"] ** 2
    )
    logg, logg_err = df["st_logg"].astype(float), np.sqrt(
        df["st_loggerr1"] ** 2 + df["st_loggerr2"] ** 2
    )
    feh, feh_err = 0, 0.1
    radius, radius_err = df["st_rad"].astype(float), np.sqrt(
        df["st_raderr1"] ** 2 + df["st_raderr2"] ** 2
    )
    mass, mass_err = df["st_mass"].astype(float), np.sqrt(
        df["st_masserr1"] ** 2 + df["st_masserr2"] ** 2
    )
    return (
        Teff,
        Teff_err,
        logg,
        logg_err,
        feh,
        feh_err,
        radius,
        radius_err,
        mass,
        mass_err,
    )


def parse_target_name(
    toiid=None, ticid=None, ctoiid=None, name=None, update_db=False
) -> Tuple:
    if toiid:
        df = get_tois(clobber=update_db)
        logger.info("Using parameters from TOI database (use --update_db to update).")
        logger.info(f"To use published parameters in NExSci, use -name=TOI-{toiid}")
        key = "TOI"
        id = str(toiid)
        idx = df[key].apply(lambda x: str(x).split(".")[0] == id)
        source = "tfop"
        target_name = f"TOI-{id.zfill(4)}"
    if ticid:
        logger.info("Using parameters from TIC catalog (use --update_db to update).")
        key = "TIC"
        id = str(ticid)
        target_name = f"TIC-{ticid}"
        source = "custom"
        Porb = float(input("Porb (d): "))
        Porberr = float(input("Porb err (d): "))
        epoch = float(input("Epoch (BJD): "))
        epocherr = float(input("Epoch err (BJD): "))
        tdur = float(input("Tdur (h): "))
        tdurerr = float(input("Tdur err (h): "))
        depth = float(input("Depth (ppm): "))
        deptherr = float(input("Depth err (ppm): "))
        cols = [
            "Period (days)",
            "Period (days) err",
            "Epoch (BJD)",
            "Epoch (BJD) err",
            "Duration (hours)",
            "Duration (hours) err",
            "Depth (ppm)",
            "Depth (ppm) err",
        ]
        df = pd.DataFrame(
            [[Porb, Porberr, epoch, epocherr, tdur, tdurerr, depth, deptherr]],
            columns=cols,
        )
        idx = [True]
    if ctoiid:
        logger.info("Using parameters from CTOI database (use --update_db to update).")
        df = get_ctois(clobber=update_db)
        key = "CTOI"
        id = ctoiid
        idx = df["TIC ID"] == int(ctoiid)
        target_name = f"CTOI-{ctoiid}"
        source = "ctoi"
    if name:
        logger.info("Using parameters from NExSci database (use --update_db to update).")
        df = get_nexsci("pscomppars", clobber=update_db)
        # df = df[df['default_flag']==1]
        df["Period (days)"] = df["pl_orbper"].astype(float)
        df["Period (days) err"] = np.sqrt(
            df["pl_orbpererr1"] ** 2 + df["pl_orbpererr2"] ** 2
        )
        df["Epoch (BJD)"] = df["pl_tranmid"].astype(float)
        df["Epoch (BJD) err"] = 0.1
        df["Depth (ppm)"] = df["pl_trandep"].astype(float) / 100 * 1e6  # % to ppm
        df["Depth (ppm) err"] = 1_000
        df["Duration (hours)"] = df["pl_trandur"].astype(float)
        df["Duration (hours) err"] = np.sqrt(
            df["pl_trandurerr1"].astype(float) ** 2
            + df["pl_trandurerr2"].astype(float) ** 2
        )
        key = "hostname"
        id = name.lower()
        target_name = name.strip().replace(" ", "")
        idx = df[key].str.lower() == id
        source = "nexsci"
    msg = f"Coulnd't find {key} {id} in {key} database."
    assert sum(idx) > 0, logger.error(msg)
    return target_name, df[idx].reset_index(drop=True), source


def get_tess_sectors(
    target_name: str, df: pd.DataFrame, toiid=None, ctoiid=None, name=None
) -> Tuple:
    if toiid or ctoiid:
        # if target is available in TOI,CTOI,or NexSci
        coord = SkyCoord(*df[["RA", "Dec"]].values[0], unit=("hourangle", "deg"))
        ra, dec = coord.ra.deg, coord.dec.deg
        ticid = df["TIC ID"].unique()[0]
    elif name:
        # for other targets
        data_json = get_tfop_info(target_name)
        ra = float(data_json["coordinates"]["ra"])
        dec = float(data_json["coordinates"]["dec"])
        ticid = int(data_json["basic_info"]["tic_id"])
    else:
        msg = "Set toiid, ctoiid, or name."
        logger.error(msg); sys.exit()
    try:
        (
            outID,
            outEclipLong,
            outEclipLat,
            outSec,
            outCam,
            outCcd,
            outColPix,
            outRowPix,
            scinfo,
        ) = tess_stars2px_function_entry(ticid, ra, dec)
    except Exception as e:
        logger.error(f"Error: {e}")
    return ticid, outSec


def check_if_sector_is_available(
    target_name: str, given_sector, all_sectors: list
) -> str:
    """
    All cases for given_sector=(None, 0, 1, 'all', [1,2], -1)
    Check only if given_sector is non-negative int or list
    """
    if given_sector is None:
        return "default"
    else:
        assert isinstance(given_sector, list)
        assert isinstance(all_sectors, np.ndarray)
        if len(given_sector) == 1:
            if given_sector == ["all"]:
                return "all_sector"
            elif given_sector == ["-1"]:
                return "last"
            elif given_sector == ["0"]:
                return "first"
        # check if given_sector exists if not 'all','0',or '-1'
        idx = np.array([True if int(s) in all_sectors else False for s in given_sector])
        msg = (
            f"{target_name} was not observed in sector={np.array(given_sector)[~idx]}\n"
        )
        msg += f"Try sector={all_sectors}."
        assert np.all(idx), logger.error(msg)
        return "multi_sector"


if __name__ == "__main__":
    ap = ArgumentParser()
    group1 = ap.add_mutually_exclusive_group(required=True)
    group1.add_argument("-toi", help="TOI ID", type=int)
    group1.add_argument("-ctoi", help="CTOI ID", type=int)
    group1.add_argument("-tic", help="TIC ID", type=int)
    group1.add_argument("-name", help="Name", type=str)
    group2 = ap.add_mutually_exclusive_group(required=True)
    group2.add_argument(
        "-s",
        "--sector",
        nargs="+",
        help="sector=-1 uses most recent TESS sector (default); try -sector=all to use all",
        default=None,
    )
    group2.add_argument(
        "-c",
        "--campaign",
        help="campaign=-1 uses most recent K2 campaign (default); try -campaign=all to use all",
        default=None,
    )
    group2.add_argument(
        "-q",
        "--quarter",
        help="quarter=-1 uses most recent Kepler quarter (default); try -quarter=all to use all",
        default=None,
    )
    # ap.add_argument("-s", "--sector", "--sector", nargs='+',
    #                 help="-sector=-1 uses most recent TESS sector (default); try -sector=all to use all",
    #                 default=None)
    ap.add_argument(
        "-e", "--exptime", help="exposure time (default=None)", type=float, default=None
    )
    ap.add_argument(
        "-p", "--pipeline", help="TESS/Kepler data pipeline (default='spoc')", type=str, default="spoc"
    )
    ap.add_argument(
        "-f", "--filename", help="filename of lightcurve (default='tess')", type=str, default="tess"
    )
    ap.add_argument(
        "-m", "--mission", help="satellite mission (default='tess')", type=str, default="tess", choices=["tess", "k2", "kepler"]
    )
    ap.add_argument(
        "-lc",
        "--lc_type",
        help="type of light curve (default=pdcsap)",
        choices=["pdcsap", "sap"],
        type=str,
        default="pdcsap",
    )
    ap.add_argument(
        "-sig",
        "--sigma",
        help="sigma for removing outliers in (combined) TESS lc",
        type=float,
        default=None,
    )
    ap.add_argument(
        "-qb",
        "--quality",
        choices=["none", "default", "hard", "hardest"],
        type=str,
        default="default",
    )
    ap.add_argument("-dir", help="base directory", type=str, default=".")
    ap.add_argument(
        "-i",
        "--interactive",
        help="manually input missing values (default=False)",
        action="store_true",
        default=False,
    )
    ap.add_argument(
        "-u",
        "--update_db",
        help="update TOI or NExSci database (default=False)",
        action="store_true",
        default=False,
    )
    ap.add_argument(
        "-r",
        "--results_dir",
        help="path to the results dir of a previous run to be used in params.csv",
        default=None,
    )
    ap.add_argument(
        "-o",
        "--overwrite",
        help="overwrite files (default=False)",
        action="store_true",
        default=False,
    )
    ap.add_argument("--debug", action="store_true", default=False)

    args = ap.parse_args(None if sys.argv[1:] else ["-h"])

    basedir = args.dir
    outdir = Path(basedir)
    results_dir = args.results_dir
    debug = args.debug

    if results_dir:
        alles = allesclass(outdir.joinpath(results_dir))
        logger.info("Updating params.csv")

        # =====Update params.csv=====
        text = """#name,value,fit,bounds,label,unit,coupled_with\n"""
        try:
            # See https://github.com/MNGuenther/allesfitter/blob/master/allesfitter/general_output.py#L935
            ll = alles.posterior_params_ll.copy() #1-sigma lower error: 16th per
            median = alles.posterior_params_median.copy() #50th percentile
            ul = alles.posterior_params_ul.copy() #1-sigma upper error: 84th perc - median
            for name,label,unit in zip(alles.BASEMENT.fitkeys,
                                       alles.BASEMENT.fitlabels,
                                       alles.BASEMENT.fitunits):
                norm_test = anderson(alles.posterior_params[name], dist='norm')
                # critical values: 15%, 10%, 5%, 2.5%, 1%
                # a crit val of 5% corresponds to 95% confidence level
                # if statistic < crit val --> fail to reject normality
                dist = 'normal' if norm_test.statistic < norm_test.critical_values[0] else 'uniform'
                if dist=='normal':
                    # or use scipy.normal.fit?
                    l_err,mid,u_err = ll[name], median[name], ul[name]
                    sig = np.sqrt(l_err**2+u_err**2)
                    text += f"{name},{mid:6f},1,normal {mid:6f} {sig:6f},{label},{unit},\n"
                    if debug:
                        logger.info(f"{mid:.6f} +{u_err:.6f} -{l_err:.6f}")
                elif dist=='uniform':                    
                    # or use scipy.uniform.fit?
                    l_limit,mid,u_limit = np.nanpercentile(alles.posterior_params[name], q=[1,50,99])
                    text += f"{name},{mid:6f},1,uniform {l_limit:6f} {u_limit:6f},{label},{unit},\n"
                    if debug:
                        logger.info(f"{l_limit:.6f} < {mid:.6f} < {u_limit:.6f}")
                else:
                    msg = "distribution is not uniform or normal!"
                    logger.error(msg); sys.exit()       
        except Exception as e:
            logger.error(f"Error: {e}")
        if debug:
            logger.info(text)
        fp = Path(results_dir, f"params2.csv")
        np.savetxt(fp, [text], fmt="%s")
        logger.info(f"Saved: {fp}")
        sys.exit()

    else:
        toiid = args.toi
        ticid = args.tic
        ctoiid = args.ctoi
        name = args.name
        exptime = args.exptime
        mission = args.mission.lower()
        quality_bitmask = args.quality
        sigma = args.sigma
        interactive = args.interactive
        fn = args.filename

        if mission == "tess":
            sector = args.sector
        elif mission == "k2":
            campaign = -1 if args.campaign is None else args.campaign
            raise NotImplementedError("TODO")
        elif mission == "kepler":
            quarter = -1 if args.quarter is None else args.quarter
            raise NotImplementedError("TODO")

        pipeline = args.pipeline
        lc_type = "sap_flux" if pipeline == "qlp" else args.lc_type + "_flux"

        overwrite = args.overwrite
        update_db = args.update_db

        target_name, target_df, source = parse_target_name(
            toiid, ticid, ctoiid, name, update_db
        )
        if ticid:
            name = target_name.replace("-", "")
        tic_id, outSec = get_tess_sectors(target_name, target_df, toiid, ctoiid, name)
        sector_flag = check_if_sector_is_available(
            target_name, given_sector=sector, all_sectors=outSec
        )

        fp = Path(basedir, target_name).joinpath(f"{target_name}.log")
        logger.add(fp, format=log_format)
        if debug:
            logger.info(target_df)

        try:
            if toiid or ctoiid or ticid:
                (
                    Teff,
                    Teff_err,
                    logg,
                    logg_err,
                    feh,
                    feh_err,
                    radius,
                    radius_err,
                    mass,
                    mass_err,
                ) = catalog_info_TIC(int(tic_id))
            elif name:
                (
                    Teff,
                    Teff_err,
                    logg,
                    logg_err,
                    feh,
                    feh_err,
                    radius,
                    radius_err,
                    mass,
                    mass_err,
                ) = catalog_info_name(target_df.iloc[0])
            if debug:
                logger.info(
                    Teff,
                    Teff_err,
                    logg,
                    logg_err,
                    feh,
                    feh_err,
                    radius,
                    radius_err,
                    mass,
                    mass_err,
                )
        except Exception as e:
            logger.error(f"Error: {e}")

        ticid = tic_id if ticid is None else ticid
        rhostar_prior = True
        if str(radius) == "nan":
            if interactive:
                radius = float(input("Rstar [Rsun]:"))
                rhostar_prior = False
            else:
                msg = "use --interactive to input value"
                logger.error(msg); sys.exit()
        if str(mass) == "nan":
            if interactive:
                mass = float(input("Mstar [Msun]:"))
                rhostar_prior = False
            else:
                msg = "use --interactive to input value"
                raise (msg)
        if str(radius_err) == "nan":
            radius_err = 0.1
            logger.info("Rstar err is nan; setting to 0.1")
        if str(mass_err) == "nan":
            mass_err = 0.1
            logger.info("Mstar err is nan; setting to 0.1")
        if debug:
            # logger.info(f"Teff={Teff:.0f}+/-{Teff_err:.0f},
            #     logg={logg:.2f}+/-{logg_err:.2f},
            #     feh={feh:.2f}+/-{feh_err:.2f}")
            logger.info(f"Rs={radius:.2f}+/-{radius_err:.2f}, Ms={mass:.2f}+/-{mass_err:.2f}")

        # band = mission.lower()
        if np.isnan(feh):
            feh = float(input("[Fe/H]: ")) if interactive else 0  # solar metallicity
        if np.isnan(feh_err):
            feh_err = float(input("[Fe/H] err: ")) if interactive else 0.1
        logger.info(f"Using [Fe/H]=({feh},{feh_err}) dex")
        if np.isnan(logg):
            if interactive:
                logg = float(input("logg: "))
            else:
                msg = "use --interactive to input value"
                logger.error(msg); sys.exit() # no assumption
        if np.isnan(logg_err):
            logg_err = float(input("logg err: ")) if interactive else 0.1
        logger.info(f"Using logg=({logg:.2f},{logg_err:.2f}) cm/s^2")
        if np.isnan(Teff):
            if interactive:
                Teff = float(input("Teff: "))
            else:
                msg = "use --interactive to input value"
                logger.error(msg); sys.exit() # no assumption
        if np.isnan(Teff_err):
            Teff_err = float(input("Teff err: ")) if interactive else 500
        logger.info(f"Using Teff=({Teff:.0f},{Teff_err:.0f}) K")

        q1, q1_err, q2, q2_err = ld.claret(
            band="T",
            teff=Teff,
            uteff=Teff_err,
            logg=logg,
            ulogg=logg_err,
            feh=feh,
            ufeh=feh_err,
            law="quadratic",
        )
        # ===== Write files =====#
        outdir = Path(basedir, target_name)
        try:
            outdir.mkdir(parents=True, exist_ok=overwrite)
        except Exception as e:
            logger.error(f"Error: {e}")
            raise FileExistsError("Use --overwrite to overwrite files.")

        # ===== Create params.csv =====#
        text = """#name,value,fit,bounds,label,unit,truth\n"""
        for i, row in target_df.iterrows():
            pl = planets[i]
            if debug:
                logger.info(f"=====Planet {pl}=====")

            # tic = row['TIC ID']
            Porb = row["Period (days)"]
            Porberr = row["Period (days) err"]
            epoch = row["Epoch (BJD)"]
            epocherr = row["Epoch (BJD) err"]
            tdur = row["Duration (hours)"]
            tdurerr = row["Duration (hours) err"]

            if interactive and not np.all([Porb > 0, epoch > 0, tdur > 0]):
                Porb = float(input("Porb: "))
                Porberr = float(input("Porb err: "))
                epoch = float(input("Epoch: "))
                epocherr = float(input("Epoch err: "))
            # tdur = float(input("Tdur: "))
            # tdurerr = float(input("Tdur err: "))
            else:
                assert np.all([Porb > 0, epoch > 0, tdur > 0])
            Porb_s = np.random.normal(Porb, Porberr, size=Nsamples)

            if debug:
                logger.info(f"P={Porb:.4f}+/-{Porberr:.4f}, T0={epoch:.4f}+/-{epocherr:.4f}")

            rprs = np.sqrt(row["Depth (ppm)"] / 1e6)
            rprserr = np.sqrt(row["Depth (ppm) err"] / 1e6)

            if str(rprs) == "nan" or str(rprserr) == "nan":
                if interactive:
                    logger.info(
                        f"rprs={row['Depth (ppm)']/1e3:.3f}, rprserr={row['Depth (ppm) err']/1e3:.3f} ppt"
                    )
                    try:
                        rprs = input(f"Planet {pl} Rp/Rs (ppt): ")
                        rprs = float(rprs) / 1e3
                        rprserr = input(f"Planet {pl} Rp/Rs err (ppt): ")
                        rprserr = float(rprserr) / 1e3
                    except Exception as e:
                        msg = f"Error in inputs.\n{e}"
                        logger.error(msg)
                elif hasattr(row, "pl_rade"):
                    try:
                        rprs = row["pl_rade"] * u.Rearth.to(u.Rsun) / row["st_rad"]
                        Rperr = np.sqrt(
                            row["pl_radeerr1"] ** 2 + row["pl_radeerr2"] ** 2
                        )
                        rprserr = Rperr * u.Rearth.to(u.Rsun) / radius_err
                    except Exception as e:
                        msg = f"Error in parsing rp/rs\n{e}"
                        logger.error(msg)
                else:
                    msg = "Rp/Rs is nan. Try --interactive for manual input"
                    logger.error(msg); sys.exit()
            assert rprs > 0

            rprs_s = np.random.normal(rprs, rprserr, size=Nsamples)
            rprs_min, rprs, rprs_max = np.percentile(rprs_s, q=quartiles_3sig)

            mass_s = np.random.normal(mass, mass_err, size=Nsamples)
            radius_s = np.random.normal(radius, radius_err, size=Nsamples)

            # compute a/Rs from stellar density
            rho_s = rho_from_mr(mass_s, radius_s)
            as_s = as_from_rhop(rho_s, Porb_s)
            if debug:
                rhomin, rho, rhomax = np.percentile(rho_s, q=quartiles_3sig)
                as_min, a, as_max = np.percentile(as_s, q=quartiles_3sig)
                a_au_s = a_from_rhoprs(rho_s, Porb_s, radius_s)
                a_au_min, a_au, a_au_max = np.percentile(a_au_s, q=quartiles_3sig)

            # FIXME: as_s produces some NaNs e.g. for Kepler-51
            idx = as_s > 0
            rsuma_s = radius_s[idx] / as_s[idx]
            # import pdb; pdb.set_trace()
            rsuma_min, rsuma, rsuma_max = np.percentile(rsuma_s, q=quartiles_3sig)
            rsuma_max = 1 if rsuma_max>1 else rsuma_max

            theta = np.arcsin(radius_s / as_s)
            inc_s = np.pi / 2 - theta
            # if all(inc_s>np.pi/2):
            #    inc_s = np.random.uniform(np.deg2rad(80), np.deg2rad(90), size=Nsamples)
            inc_min, inc, inc_max = np.percentile(inc_s, q=quartiles_3sig)
            b_s = as_s * np.cos(inc_s)
            b_min, b, b_max = np.percentile(b_s, q=quartiles_3sig)
            if all(b_s > 1):
                b_s = np.random.uniform(0, 1, size=Nsamples)
                b = 0

            if str(tdur) != "nan":
                # check if tdur derived from rhostar is consistent with tdur from tfop
                tdur_rhostar = get_tdur(Porb, rsuma, inc, rprs, b) * 24
                logger.info(
                    f"tdur={tdur:.1f}h ({source}) {tdur_rhostar:.1f}h (derived from rhostar)"
                )
                # check if tdur derived from orbit is consistent with tdur from tfop
                try:
                    tdurerr = 1 if str(tdurerr) == "nan" else tdurerr
                    tdur_s = np.random.normal(tdur, tdurerr, size=Nsamples) / 24
                    rsuma_s = get_rsuma(tdur_s, Porb, inc_s, rprs_s, b_s)
                    tdur_orbit = get_tdur(Porb, np.median(rsuma_s), inc, rprs, b) * 24
                    logger.info(
                        f"tdur={tdur:.1f}h ({source}) {tdur_orbit:.1f}h (derived from orbit)"
                    )
                    if (
                        np.nanargmin(
                            np.abs(np.array([tdur_rhostar, tdur_orbit]) - tdur)
                        )
                        == 1
                    ):
                        logger.info("Using Rstar/a derived from orbit.")
                        rsuma_min, rsuma, rsuma_max = np.percentile(
                            rsuma_s, q=quartiles_3sig
                        )
                    else:
                        logger.info("Using Rstar/a derived from rhostar.")
                except Exception as e:
                    logger.error(f"Error: {e}")

            if debug:
                logger.info(f"rsuma={rsuma:.4f}")
                logger.info(f"rprs={rprs:.4f}")
                logger.info(f"rho={rho:.4f}")
                logger.info(f"a_s={a:.4f}")
                logger.info(f"a_au={a_au:.4f}")
                logger.info(f"inc={np.rad2deg(inc):.2f}")
                logger.info(f"b={b:.2f}")
            text += f"#companion {pl} astrophysical params,,,,,,\n"
            text += f"{pl}_rr,{rprs:.4f},1,uniform 0 {ceil(rprs_max*10)/10:.4f},$R_{pl} / R_\star$,,\n"
            text += f"{pl}_rsuma,{rsuma:.4f},1,uniform {rsuma_min:.4f} {ceil(rsuma_max*10)/10:.4f},$(R_\star + R_{pl}) / a_{pl}$,,\n"
            #text += f"{pl}_rsuma,{rsuma:.4f},1,uniform 0 1,$(R_\star + R_{pl}) / a_{pl}$,,\n"
            text += f"{pl}_cosi,0,1,uniform 0 1,$\cos" + "{i_" + pl + "}$,,\n"
            text += (
                f"{pl}_epoch,{epoch:.6f},1,normal {epoch:.6f} {epocherr:.6f},$T_"
                + "{0;"
                + pl
                + "}$,BJD,\n"
            )
            text += (
                f"{pl}_period,{Porb:.6f},1,normal {Porb:.6f} {Porberr:.6f},$P_{pl}$,d,\n"
            )
            text += f"{pl}_f_c,0,0,uniform -1 1,$\sqrt{{e_{pl}}} \cos{{\omega_{pl}}}$,,\n"
            text += f"{pl}_f_s,0,0,uniform -1 1,$\sqrt{{e_{pl}}} \sin{{\omega_{pl}}}$,,\n"
        text += "#dilution per instrument,,,,,,\n"
        text += f"dil_{fn},0,0,uniform -1 1,$D_\mathrm{{0; {fn}}}$,,\n"
        text += "#limb darkening coefficients per instrument,,,,,,\n"
        text += (
            f"#host_ldc_q1_{fn},{q1:.2f},1,normal {q1:.2f} {q1_err:.2f},"
            + f"$q_{{1; \\mathrm{{{fn}}}}}$"
            + ",,\n"
        )
        text += (
            f"#host_ldc_q2_{fn},{q2:.2f},1,normal {q2:.2f} {q2_err:.2f},"
            + f"$q_{{2; \\mathrm{{{fn}}}}}$"
            + ",,\n"
        )
        text += f"host_ldc_q1_{fn},0.5,1,uniform 0 1,$q_{{1; \\mathrm{{{fn}}}}}$,,\n"
        text += f"host_ldc_q2_{fn},0.5,1,uniform 0 1,$q_{{2; \\mathrm{{{fn}}}}}$,,\n"
        
        text += "#errors per instrument,,,,,,\n"
        text += (
            f"ln_err_flux_{fn},-6,1,uniform -10 -1,$\log{{\sigma ({fn})}}$,rel. flux,\n"
        )
        text += "#baseline per instrument,,,,,,\n"
        text += f"baseline_gp_offset_flux_{fn},0,1,uniform -0.1 0.1,$\mathrm{{gp ln \sigma ({fn})}}$,,\n"
        text += f"baseline_gp_matern32_lnsigma_flux_{fn},-5,1,uniform -15 0,$\mathrm{{gp ln \sigma ({fn})}}$,,\n"
        text += f"baseline_gp_matern32_lnrho_flux_{fn},0,1,uniform -1 15,$\mathrm{{gp ln \\rho ({fn})}}$,,\n"
        for i, row in target_df.iterrows():
            pl = planets[i]
            text += f"#TTV companion {pl},,,,,\n"
            for i in range(5):
                text += f"#{pl}_ttv_transit_{i+1},0,1,uniform -0.1 0.1,TTV$_\mathrm{{ttv;{i+1}}}$,d,\n"
        if debug:
            logger.info(text)
        fp = outdir.joinpath("params.csv")
        np.savetxt(fp, [text], fmt="%s")
        logger.info(f"Saved: {fp}")

        # =====Create settings.csv===== #
        text2 = """#name,value
###############################################################################,
# General settings,
###############################################################################,\n"""

        text2 += f"companions_phot,{' '.join(planets[:len(target_df)])}"

        text2 += f"""
companions_rv,
inst_phot,{fn}
inst_rv,
###############################################################################,
# Fit performance settings,
###############################################################################,
multiprocess,True
multiprocess_cores,40
fast_fit,True
fast_fit_width,0.3333333333333333
#fast_fit_width,0.5
secondary_eclipse,False
phase_curve,False
shift_epoch,True\n"""
        for i, row in target_df.iterrows():
            pl = planets[i]
            text2 += f"inst_for_{pl}_epoch,all\n"
            text2 += f"#inst_for_{pl}_epoch,{fn}\n"
        text2 += f"""###############################################################################,
# MCMC settings,
###############################################################################,
mcmc_nwalkers,100
mcmc_total_steps,2000
mcmc_burn_steps,1000
mcmc_thin_by,2
###############################################################################,
# Nested Sampling settings,
###############################################################################,
ns_modus,dynamic
ns_nlive,1000
ns_bound,single
ns_sample,auto
ns_tol,100
###############################################################################,
# Limb darkening law per object and instrument,
# if 'quad' two corresponding parameter called 'ldc_q1_inst' and 'ldc_q2_inst' have to be given in params.csv,
###############################################################################,
host_ld_law_{fn},quad
#####################################,
# Exposure interpolation settings,
#####################################,
### crucial only for long (>600s) exposure times,
#t_exp_{fn},0.0208333
#t_exp_n_int_{fn},10
###############################################################################,
# Baseline settings per instrument: sample / hybrid,
# if 'sample_offset' one corresponding parameter called 'baseline_offset_key_inst' has to be given in params.csv,
# if 'sample_linear' two corresponding parameters called 'baseline_offset_key_inst' and 'baseline_slope_key_inst' have to be given in params.csv,
# if 'sample_GP' two corresponding parameters called 'baseline_gp1_key_inst' and 'baseline_gp2_key_inst' have to be given in params.csv,
###############################################################################,
#baseline_flux_{fn},sample_offset
#baseline_flux_{fn},sample_linear
#baseline_flux_{fn},hybrid_spline
#baseline_flux_{fn},hybrid_poly_2
baseline_flux_{fn},sample_GP_Matern32
###############################################################################,
# Error settings (overall scaling) per instrument: sample / hybrid,
# if 'sample' one corresponding parameter called 'ln_err_key_inst' (photometry) or 'ln_jitter_key_inst' (RV) has to be given in params.csv,
###############################################################################,
error_flux_{fn},sample
###############################################################################,
# Flares,
# if N>0 4xN corresponding parameters has to be given in params.csv,
# See https://github.com/MNGuenther/allesfitter/blob/master/paper/GJ_1243/allesfit_0/params.csv,
###############################################################################,
#N_flares,0
###############################################################################,
# Number of spots per object and instrument,
# if N>0 3xN corresponding parameters has to be given in params.csv,
###############################################################################,
#host_N_spots_{fn},0
###############################################################################,
# Host density prior,
###############################################################################,\n"""
        text2 += f"use_host_density_prior,{rhostar_prior}"
        text2 += f"""
###############################################################################,
# Stellar variability: sample_GP_SHO / _real / _complex / matern32,
# if 'sample_GP_SHO' three corresponding parameters has to be given in params.csv,
# See https://github.com/MNGuenther/allesfitter/blob/master/tutorials/06_transits_and_rvs_with_stellar_variability/allesfit/params.csv,
###############################################################################,
#stellar_var_flux,sample_GP_SHO
#stellar_var_rv,sample_GP_real
###################################################,
# Fit TTV,
###################################################,
fit_ttvs,False
###############################################################################,
# Stellar grid per object and instrument,
###############################################################################,
host_grid_{fn},very_sparse\n"""
        for i, row in target_df.iterrows():
            pl = planets[i]
            text2 += f"#{pl}_grid_{fn},very_sparse\n"
            text2 += f"#{pl}_shape_{fn},sphere\n"
            text2 += f"#{pl}_flux_weighted_{fn},False\n"

        if debug:
            logger.info(text2)

        fp = outdir.joinpath("settings.csv")
        np.savetxt(fp, [text2], fmt="%s")
        logger.info(f"Saved: {fp}")

        # =====Create params_star.csv===== #
        text3 = f"""#R_star,R_star_lerr,R_star_uerr,M_star,M_star_lerr,M_star_uerr,Teff_star,Teff_star_lerr,Teff_star_uerr
    #R_sun,R_sun,R_sun,M_sun,M_sun,M_sun,K,K,K
    {radius:.2f},{radius_err:.2f},{radius_err:.2f},{mass:.2f},{mass_err:.2f},{mass_err:.2f},{Teff:.0f},{Teff_err:.0f},{Teff_err:.0f}"""
        if debug:
            logger.info(text3)

        fp = outdir.joinpath("params_star.csv")
        np.savetxt(fp, [text3], fmt="%s")
        logger.info(f"Saved: {fp}")

        # =====Create run.py===== #
        text4 = """#!/usr/bin/env python
import allesfitter

fig = allesfitter.show_initial_guess('.')
#allesfitter.prepare_ttv_fit('.', style='tessplot')

# nested sampling
#allesfitter.ns_fit('.')
#allesfitter.ns_output('.')

# mcmc (if needed)
#allesfitter.mcmc_fit('.')
#allesfitter.mcmc_output('.')"""

        if debug:
            logger.info(text4)

        fp = outdir.joinpath("run.py")
        np.savetxt(fp, [text4], fmt="%s")
        logger.info(f"Saved: {fp}")

        query_name = name
        if toiid or ctoiid:
            query_name = f"TIC {ticid}"
        elif name.lower()[:2] == "k2":
            # search epic name or coordinates
            try:
                logger.info("Searching for EPIC name")
                query_name = get_name_aliases(name, key="epic")
            except Exception as e:
                logger.info(f"Error: {e}")

        # search all available data for reference
        all_lcs = lk.search_lightcurve(query_name, mission=mission)
        logger.info(all_lcs)  
        if len(all_lcs) > 0:
            pipelines = set([i.lower() for i in all_lcs.author])
            logger.info(f"Available Pipelines: {pipelines}")
        else:
            msg = "No light curves found."
            logger.error(msg); sys.exit()               
        unique_exptimes = all_lcs.table.to_pandas().exptime.unique()
        logger.info(f"Available Exp. times: {unique_exptimes}")
        idx = [i == pipeline.lower() for i in pipelines]
        if sum(idx) == 0:
            msg = f"pipeline={pipeline} not in {pipelines}"
            logger.error(msg); sys.exit()
        
        # search only requested data
        result = lk.search_lightcurve(
            query_name, author=pipeline, exptime=exptime, mission=mission
        )
        if result:
            sectors = list(map(int, [s.split()[-1] for s in result.mission]))
            unique_sectors = sorted(set(sectors))
            if sector_flag == "all_sector":
                # case: sector='all'
                logger.info(
                    f"Using {pipeline.upper()} pipeline in {len(sectors)} sectors: {sectors}"
                )
                unique_exptimes = result.table.to_pandas().exptime.unique()
                if len(unique_exptimes) > 1:
                    msg = f"Multiple exposure times are available for `all` sectors:\n{result}.\n"
                    msg += f"Try using -exp={unique_exptimes}"
                    logger.error(msg); sys.exit()
                exptime = unique_exptimes[0] if exptime is None else exptime
                lc = result.download_all(
                    flux_column=lc_type, quality_bitmask=quality_bitmask
                ).stitch()
                logger.info(
                    "The lightcurves were not flattened/de-trended to avoid removing transits."
                )
                assert lc.sector == int(unique_sectors[-1])
                if pipeline == "spoc":
                    lc1 = result.download_all(
                        quality_bitmask=quality_bitmask, flux_column="pdcsap_flux"
                    ).stitch()
                    lc2 = result.download_all(
                        quality_bitmask=quality_bitmask, flux_column="sap_flux"
                    ).stitch()
            elif sector_flag == "multi_sector":
                # case: sector int or list
                idx = [str(i) in sector for i in sectors]
                msg = f"{pipeline.upper()} lightcurves for sector={sector} is not available. Try sector={unique_sectors}."
                assert sum(idx) > 0, logger.error(msg)

                filtered_result = result[idx]
                unique_exptimes = filtered_result.table.to_pandas().exptime.unique()
                msg = f"Using {pipeline.upper()} pipeline in {len(sector)} sectors: {sector} (exptime={unique_exptimes} sec).\n"
                if sector_flag != "all_sector":
                    msg += f"Otherwise use sector=({unique_sectors}, all))."
                logger.info(msg)
                if len(sector) > len(filtered_result):
                    msg = f"Not all sector={sector} have exptime={exptime} sec.\n"
                    msg = "Try to limit the sectors.\n"
                    logger.error(msg); sys.exit()
                elif len(sector) < len(filtered_result):
                    msg = f"Multiple exposure times are available for the given sector:\n{filtered_result}.\n"
                    msg += f"Try using -exp={unique_exptimes}"
                    logger.error(msg); sys.exit()
                assert len(sector) == len(filtered_result)
                exptime = unique_exptimes[0] if exptime is None else exptime
                lc = filtered_result.download_all(
                    quality_bitmask=quality_bitmask, flux_column=lc_type
                ).stitch()
                logger.info(
                    "The lightcurves were not flattened/de-trended to avoid removing transits."
                )
                msg = f"sector={lc.sector} in header not in requested sector={sector}"
                assert str(lc.sector) in np.array(sector), logger.error(msg)
                if pipeline == "spoc":
                    lc1 = filtered_result.download_all(
                        quality_bitmask=quality_bitmask, flux_column="pdcsap_flux"
                    ).stitch()
                    lc2 = filtered_result.download_all(
                        quality_bitmask=quality_bitmask, flux_column="sap_flux"
                    ).stitch()
            else:
                if sector_flag == "first":
                    idx = 0
                    sector = sectors[idx]
                elif sector_flag == "last" or sector_flag == "default":
                    idx = -1
                    sector = sectors[idx]

                filtered_result = result[idx]
                lc = filtered_result.download(
                    quality_bitmask=quality_bitmask, flux_column=lc_type
                ).normalize()
                unique_exptimes = filtered_result.table.to_pandas().exptime.unique()
                # logger.info(f"Exp times: {unique_exptimes}")
                exptime = unique_exptimes[0] if exptime is None else exptime
                assert lc.sector == sector
                logger.info(f"Using {pipeline.upper()} pipeline in sector {sector}.")
                if pipeline == "spoc":
                    lc1 = filtered_result.download(
                        quality_bitmask=quality_bitmask, flux_column="pdcsap_flux"
                    ).normalize()
                    lc2 = filtered_result.download(
                        quality_bitmask=quality_bitmask, flux_column="sap_flux"
                    ).normalize()
            if sigma:
                nbefore = len(lc)
                lc = lc.remove_outliers(sigma=sigma)
                nafter = len(lc)
                if nbefore>nafter:
                    diff = nbefore-nafter
                    logger.info(f"Removed {diff} outliers using sigma={sigma}.")
                if pipeline == "spoc":
                    lc1 = lc1.remove_outliers(sigma=sigma)
                    lc2 = lc2.remove_outliers(sigma=sigma)
            if sector_flag == "all_sector":
                secs = "s".join(map(str, unique_sectors))
            else:
                if isinstance(sector, list):
                    secs = "s".join(map(str, sector))
                else:
                    secs = str(sector)
            if pipeline == "spoc" and len(lc1)==len(lc2):
                fig, axs = plt.subplots(2, 1, figsize=(8,6), sharex=True)
                _ = lc1.scatter(ax=axs[0], zorder=2, label="PDCSAP", c="C0")
                _ = lc2.scatter(ax=axs[0], zorder=1, label="SAP", c="C1")
                axs[0].set_title(f"Sector={secs}\nexptime={int(exptime)}s")
                (lc1-lc2).scatter(ax=axs[1], label='difference', c='k')
                fp = outdir.joinpath(
                    f"{target_name}_{mission}_{lc_type.split('_')[0]}_s{secs}_exp{int(exptime)}s"
                )
                fig.savefig(fp.with_suffix('.png'))
            else:
                ax = lc.scatter(label=pipeline)
                ax.set_title(f"Sector={secs}\nexptime={int(exptime)}s")
                fp = outdir.joinpath(
                    f"{target_name}_{mission}_{pipeline}_s{secs}_exp{int(exptime)}s"
                )
                ax.figure.savefig(fp.with_suffix('.png'))
            logger.info(f"Saved: {fp.with_suffix('.png')}")
            df = lc.to_pandas()
            msg = "Somehow, the lightcurve data is empty."
            assert len(df)>0, logger.error(msg)
            df["time"] = df.index + 2457000
            df = df.reset_index(drop=True).sort_values(by="time")
            df2 = df[cols].dropna(axis='index')
            msg = f"Somehow, one of the data columns is all NaN.\n{df[cols]}\n"
            msg += "No lightcurve downloaded."
            assert len(df2)>0, logger.error(msg)
            df2.to_csv(fp.with_suffix('.csv'), sep=",", header=False, index=False)
            logger.info(f"Saved: {fp.with_suffix('.csv')}")
            logger.info(f"Ndata: {len(df):,}")
            if debug:
                logger.info(df.head())
        else:
            msg = "No lightcurve downloaded. Check inputs."
            logger.error(msg); sys.exit()
